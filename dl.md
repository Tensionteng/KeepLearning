# 引言
## 机器学习分类
### 监督学习
- 回归：平方误差
- 分类/多项分类：交叉熵
- 标记/多标签分类
- 搜索、推荐系统
- 序列学习，如翻译，语音等

### 无监督学习
- 聚类
- 主成分分析
- 因果关系/概率图模型
- 对抗性网络

### 强化学习
- 马尔可夫决策过程（markov decision process）：当环境可被完全观察到时
- 上下文赌博机（contextual bandit problem）：当状态不依赖于之前的操作时
- 多臂赌博机（multi-armed bandit problem）：当没有状态，只有一组最初未知回报的可用动作时

# 卷积神经网络
- 暂退法和计算weight的二范式都是很好的正则化方法
- padding大小为$\frac{kernel\_seize-1}{2}$最佳，可以保持分辨率，即输入输出的张量形状是一样的,torch中，padding是上下左右都补全的，如padding=2，会导致张量上下左右都+2
- `nn.Conv2d(in, out)`中，in是指输入的通道数，out是指输出的通道数，一个卷积层的参数数量为(in * out * kernel_size * kernel_size)

- `(1 * 1)`的卷积核可以改变通道数，降低模型复杂度
- 卷积计算公式: 输出高度 = （输入高度 - Kernel高度 + 2 * padding）/ 步长stride + 1，除法为向下取整
- `BatchNormal(num_feature)`，参数为样本大小，如果是全链接层，那就对应批量大小（因此批量不能为1，否则会减去平均值之后会全部变成0）；如果是卷积层，对应通道数量。批量规范化层一般存在于全输出层或者卷积层之后，激活函数之前，能让中间输出的值更稳定

## google-net
将多个不同维度的卷积核得到的输出堆叠在一起，作为最终的输出。这样的好处有两个，第一，不同的卷积核有不同的感受野，可以学习到不同的特征；第二，大的卷积核能够显著减少参数的数量

## res-net
在深度网络中，很难训练出恒等函数，导致网络退化。res-net的思想在于，将训练恒等函数转为训练残差函数，残差函数的变化率较大，对于深度网络来说，训练起来比较容易。例如，原本训练出的函数为F(x)，残差函数为H(x)=F(x)-x，F(5)=5.1，H(5)=F(5)-5=0.1，5到0.1相差了10倍，深度网络能够比较好地训练出这样的映射。
训练出H(x)后，只需要在最后加上x，即可模拟恒等映射，即H(x)+x= F(x)

在后面的论文中，He提出了改良版的res-net架构，即先BatchNormal再ReLU，再卷积（原本的结构是，先卷积再BatchNormal最后再ReLU）

# 循环神经网络
## 困惑度
困惑度是衡量一个模型好坏的指标。要理解困惑度，要先理解如何计算一个句子的概率。把一个句子分割成n个词元（token），记为x1，x2，x3，...，xn，那么一个句子的概率就是p（x1）p（x2｜x1）p（x3｜x2，x1）...p（xn｜xn-1,xn-2,...,x1）
由于p（xn）都是小数，为了防止溢出，需要将最后的结果乘log（机器学习框架一般是10底或者e底），句子概率最大越好
在训练模型的过程中，需要一个越小越好的损失函数，因此就有了**困惑度**
先看看困惑度的定义：
$$perplexity(W)={p(x_1,x_2,x_3,...,x_n)}^{-\frac{1}{N}}$$
其中，$p(x_1,x_2,x_3,...,x_n)$是句子概率